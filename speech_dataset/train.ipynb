{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde3a6c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "ğŸŒŸ SageMaker Session Initialized!\n",
      "ğŸ“¦ S3 Bucket: pan-sea-khmer-speech-dataset-sg\n",
      "ğŸŒ Region: ap-southeast-1\n",
      "ğŸ”‘ Role: arn:aws:iam::248619912656:role/AmazonSageMaker-ExecutionRole\n",
      "ğŸ“ Prefix: khmer-whisper-training\n",
      "\n",
      "ğŸ“Š S3 Paths:\n",
      "   Dataset: s3://pan-sea-khmer-speech-dataset-sg/khmer-whisper-dataset/data\n",
      "   Models: s3://pan-sea-khmer-speech-dataset-sg/khmer-whisper-training/model-artifacts\n",
      "   Logs: s3://pan-sea-khmer-speech-dataset-sg/khmer-whisper-training/logs\n",
      "âœ… Dataset verified in S3: 5 files found\n",
      "ğŸ“‚ Sample files:\n",
      "   - khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_0_0.wav\n",
      "   - khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_0_1.wav\n",
      "   - khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_0_2.wav\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize SageMaker session for Singapore region\n",
    "sagemaker_session = sagemaker.Session(boto3.Session(region_name='ap-southeast-1'))\n",
    "role = get_execution_role()\n",
    "region = 'ap-southeast-1'  # Singapore region\n",
    "\n",
    "# Use your existing S3 bucket with uploaded dataset\n",
    "bucket = 'pan-sea-khmer-speech-dataset-sg'  # Your Singapore bucket\n",
    "prefix = 'khmer-whisper-training'\n",
    "\n",
    "print(f\"ğŸŒŸ SageMaker Session Initialized!\")\n",
    "print(f\"ğŸ“¦ S3 Bucket: {bucket}\")\n",
    "print(f\"ğŸŒ Region: {region}\")\n",
    "print(f\"ğŸ”‘ Role: {role}\")\n",
    "print(f\"ğŸ“ Prefix: {prefix}\")\n",
    "\n",
    "# Create S3 paths - Updated with your actual uploaded dataset path\n",
    "dataset_s3_path = 's3://pan-sea-khmer-speech-dataset-sg/khmer-whisper-dataset/data'  # Your uploaded dataset\n",
    "model_artifacts_path = f's3://{bucket}/{prefix}/model-artifacts'\n",
    "logs_path = f's3://{bucket}/{prefix}/logs'\n",
    "\n",
    "print(f\"\\nğŸ“Š S3 Paths:\")\n",
    "print(f\"   Dataset: {dataset_s3_path}\")\n",
    "print(f\"   Models: {model_artifacts_path}\")\n",
    "print(f\"   Logs: {logs_path}\")\n",
    "\n",
    "# Verify dataset exists\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket='pan-sea-khmer-speech-dataset-sg',\n",
    "        Prefix='khmer-whisper-dataset/data/',\n",
    "        MaxKeys=5\n",
    "    )\n",
    "    if 'Contents' in response:\n",
    "        print(f\"âœ… Dataset verified in S3: {len(response.get('Contents', []))} files found\")\n",
    "        print(f\"ğŸ“‚ Sample files:\")\n",
    "        for obj in response['Contents'][:3]:\n",
    "            print(f\"   - {obj['Key']}\")\n",
    "    else:\n",
    "        print(\"âŒ No files found in dataset path\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not verify dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635fb33-856e-4c0b-9731-7cd2213444b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Installing Whisper Dependencies for SageMaker\n",
      "==================================================\n",
      "ğŸµ Installing librosa via conda...\n",
      "âœ… Librosa installed via conda\n",
      "\n",
      "ğŸ“¦ Installing other packages...\n",
      "Installing: torch torchvision torchaudio\n",
      "âœ… torch torchvision torchaudio\n",
      "Installing: transformers==4.35.2\n",
      "âœ… transformers==4.35.2\n",
      "Installing: datasets==2.14.7\n",
      "âœ… datasets==2.14.7\n",
      "Installing: soundfile\n",
      "âœ… soundfile\n",
      "Installing: evaluate\n",
      "âœ… evaluate\n",
      "Installing: jiwer\n",
      "âœ… jiwer\n",
      "Installing: accelerate\n",
      "âœ… accelerate\n",
      "Installing: boto3\n",
      "âœ… boto3\n",
      "Installing: tqdm\n",
      "âœ… tqdm\n",
      "\n",
      "ğŸ” Testing critical imports...\n",
      "âœ… PyTorch - Working\n",
      "âœ… Transformers - Working\n",
      "âœ… Datasets - Working\n",
      "âœ… Librosa - Working\n",
      "âœ… SoundFile - Working\n",
      "âœ… Evaluate - Working\n",
      "âœ… Boto3 - Working\n",
      "âœ… Accelerate - Working\n",
      "\n",
      "ğŸ“Š Status: 8/8 critical packages working\n",
      "\n",
      "ğŸ¯ Key Versions:\n",
      "   Python: 3.10.18\n",
      "   PyTorch: 2.6.0+cu124\n",
      "   Transformers: 4.35.2\n",
      "   CUDA Available: True\n",
      "   GPU: Tesla T4\n",
      "   GPU Memory: 14.6 GB\n",
      "\n",
      "ğŸ‰ SUCCESS: Ready for Whisper training!\n",
      "ğŸ’¡ Dependency warnings are normal and won't affect training\n",
      "ğŸš€ You can proceed with the training cells\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Final SageMaker Installation - Fix Version Compatibility\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress dependency warnings\n",
    "\n",
    "print(\"ğŸš€ Installing Whisper Dependencies for SageMaker (Fixed Versions)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Essential packages with compatible versions\n",
    "def install_essential():\n",
    "    \"\"\"Install only what's needed for Whisper training with compatible versions\"\"\"\n",
    "    \n",
    "    # Core packages with version compatibility\n",
    "    essential_packages = [\n",
    "        \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",\n",
    "        \"transformers==4.35.2\",\n",
    "        \"datasets==2.14.7\", \n",
    "        \"accelerate==0.25.0\",  # Compatible version with transformers 4.35.2\n",
    "        \"soundfile\",\n",
    "        \"evaluate==0.4.1\",\n",
    "        \"jiwer==3.0.3\", \n",
    "        \"boto3\",\n",
    "        \"tqdm\",\n",
    "        \"tensorboard\"\n",
    "    ]\n",
    "    \n",
    "    # Try conda for librosa first (most reliable)\n",
    "    print(\"ğŸµ Installing librosa via conda...\")\n",
    "    try:\n",
    "        subprocess.run(\"conda install -c conda-forge librosa -y\", shell=True, check=True, capture_output=True)\n",
    "        print(\"âœ… Librosa installed via conda\")\n",
    "    except:\n",
    "        print(\"âš ï¸ Conda librosa failed, trying pip...\")\n",
    "        subprocess.run(\"pip install librosa==0.10.1\", shell=True)\n",
    "    \n",
    "    # Install other packages with version constraints\n",
    "    print(\"\\nğŸ“¦ Installing packages with compatible versions...\")\n",
    "    for package in essential_packages:\n",
    "        try:\n",
    "            cmd = f\"pip install {package}\"\n",
    "            print(f\"Installing: {package.split()[0]}\")\n",
    "            result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)\n",
    "            print(f\"âœ… {package.split()[0]}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âš ï¸ Warning: {package.split()[0]} installation issue:\")\n",
    "            print(f\"   {e.stderr[:100] if e.stderr else 'Unknown error'}\")\n",
    "\n",
    "# Run installation\n",
    "install_essential()\n",
    "\n",
    "# Test imports (the important ones)\n",
    "print(\"\\nğŸ” Testing critical imports...\")\n",
    "\n",
    "def test_package(name, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = name\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"âœ… {name} - Working\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ {name} - Failed: {str(e)[:50]}...\")\n",
    "        return False\n",
    "\n",
    "# Test critical packages for Whisper\n",
    "critical_tests = [\n",
    "    (\"PyTorch\", \"torch\"),\n",
    "    (\"Transformers\", \"transformers\"),\n",
    "    (\"Datasets\", \"datasets\"), \n",
    "    (\"Accelerate\", \"accelerate\"),\n",
    "    (\"Librosa\", \"librosa\"),\n",
    "    (\"SoundFile\", \"soundfile\"),\n",
    "    (\"Evaluate\", \"evaluate\"),\n",
    "    (\"Boto3\", \"boto3\")\n",
    "]\n",
    "\n",
    "working_count = 0\n",
    "for name, import_name in critical_tests:\n",
    "    if test_package(name, import_name):\n",
    "        working_count += 1\n",
    "\n",
    "print(f\"\\nğŸ“Š Status: {working_count}/{len(critical_tests)} critical packages working\")\n",
    "\n",
    "# Show versions of working packages\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import accelerate\n",
    "    print(f\"\\nğŸ¯ Key Versions:\")\n",
    "    print(f\"   Python: {sys.version.split()[0]}\")\n",
    "    print(f\"   PyTorch: {torch.__version__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    print(f\"   Accelerate: {accelerate.__version__}\")\n",
    "    print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        \n",
    "    # Check for compatibility\n",
    "    from packaging import version\n",
    "    trans_version = version.parse(transformers.__version__)\n",
    "    accel_version = version.parse(accelerate.__version__)\n",
    "    \n",
    "    print(f\"\\nğŸ”§ Compatibility Check:\")\n",
    "    if trans_version >= version.parse(\"4.35.0\") and accel_version >= version.parse(\"0.25.0\"):\n",
    "        print(\"âœ… Transformers and Accelerate versions are compatible\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Version mismatch detected - may cause training issues\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not get version info: {e}\")\n",
    "\n",
    "# Final status\n",
    "if working_count >= 6:  # At least 6/8 critical packages\n",
    "    print(\"\\nğŸ‰ SUCCESS: Ready for Whisper training!\")\n",
    "    print(\"ğŸ’¡ Version compatibility issues fixed\")\n",
    "    print(\"ğŸš€ You can proceed with the training cells\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some packages need manual fixes:\")\n",
    "    print(\"   Try running this cell again if issues persist\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1371d9d-2f3d-4388-9dfa-001db98d4c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” S3 Bucket Explorer\n",
      "==================================================\n",
      "ğŸ“Š Total objects found: 110558\n",
      "\n",
      "ğŸ“ Folder Structure:\n",
      "   khmer-whisper-dataset/data/test\n",
      "   khmer-whisper-dataset/data/test/audio\n",
      "   khmer-whisper-dataset/data/train\n",
      "   khmer-whisper-dataset/data/train/audio\n",
      "   khmer-whisper-dataset/data/validation\n",
      "   khmer-whisper-dataset/data/validation/audio\n",
      "\n",
      "ğŸ“„ File Types:\n",
      "   .csv: 3 files\n",
      "   .wav: 110,555 files\n",
      "\n",
      "ğŸ“‹ Manifest Files Found:\n",
      "   khmer-whisper-dataset/data/test/test_manifest.csv\n",
      "   khmer-whisper-dataset/data/train/train_manifest.csv\n",
      "   khmer-whisper-dataset/data/validation/validation_manifest.csv\n",
      "\n",
      "ğŸ“ Sample Files (first 10):\n",
      "    1. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_0_0.wav (0.15 MB)\n",
      "    2. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_0_1.wav (0.15 MB)\n",
      "    3. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_0_2.wav (0.13 MB)\n",
      "    4. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_1.wav (0.13 MB)\n",
      "    5. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_10.wav (0.08 MB)\n",
      "    6. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_100.wav (0.08 MB)\n",
      "    7. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_101.wav (0.11 MB)\n",
      "    8. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_102_0.wav (0.15 MB)\n",
      "    9. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_102_1.wav (0.11 MB)\n",
      "   10. khmer-whisper-dataset/data/test/audio/01a47357-e136-4e15-8842-e84a1f6707c4_103_0.wav (0.15 MB)\n",
      "\n",
      "ğŸ¯ Split Analysis:\n",
      "   Train:\n",
      "     Audio files: 90,606\n",
      "     Manifest files: 1\n",
      "     Manifest path: khmer-whisper-dataset/data/train/train_manifest.csv\n",
      "   Validation:\n",
      "     Audio files: 9,973\n",
      "     Manifest files: 1\n",
      "     Manifest path: khmer-whisper-dataset/data/validation/validation_manifest.csv\n",
      "   Test:\n",
      "     Audio files: 10,361\n",
      "     Manifest files: 1\n",
      "     Manifest path: khmer-whisper-dataset/data/test/test_manifest.csv\n",
      "\n",
      "ğŸ’¡ Next Steps:\n",
      "1. Check if manifest files exist at the expected paths\n",
      "2. If no manifests found, we'll need to create them or use a different loading approach\n",
      "3. The dataset loader will fallback to dummy data if needed for testing\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” S3 Bucket Explorer - Run this to see your exact S3 structure\n",
    "import boto3\n",
    "\n",
    "def explore_s3_bucket():\n",
    "    \"\"\"Explore the S3 bucket structure to understand the dataset layout\"\"\"\n",
    "    print(\"ğŸ” S3 Bucket Explorer\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    s3_client = boto3.client('s3', region_name='ap-southeast-1')\n",
    "    bucket_name = 'pan-sea-khmer-speech-dataset-sg'\n",
    "    prefix = 'khmer-whisper-dataset/data'\n",
    "    \n",
    "    try:\n",
    "        # Get all objects with prefix\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "        \n",
    "        all_objects = []\n",
    "        for page in pages:\n",
    "            all_objects.extend(page.get('Contents', []))\n",
    "        \n",
    "        print(f\"ğŸ“Š Total objects found: {len(all_objects)}\")\n",
    "        \n",
    "        # Analyze structure\n",
    "        folders = set()\n",
    "        file_types = {}\n",
    "        manifest_files = []\n",
    "        \n",
    "        for obj in all_objects:\n",
    "            key = obj['Key']\n",
    "            \n",
    "            # Track folders\n",
    "            parts = key.split('/')\n",
    "            if len(parts) > 1:\n",
    "                folder_path = '/'.join(parts[:-1])\n",
    "                folders.add(folder_path)\n",
    "            \n",
    "            # Track file types\n",
    "            if '.' in key:\n",
    "                ext = key.split('.')[-1].lower()\n",
    "                file_types[ext] = file_types.get(ext, 0) + 1\n",
    "            \n",
    "            # Track manifest files\n",
    "            if 'manifest' in key.lower():\n",
    "                manifest_files.append(key)\n",
    "        \n",
    "        print(f\"\\nğŸ“ Folder Structure:\")\n",
    "        for folder in sorted(folders)[:20]:  # Show first 20 folders\n",
    "            print(f\"   {folder}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“„ File Types:\")\n",
    "        for ext, count in sorted(file_types.items()):\n",
    "            print(f\"   .{ext}: {count:,} files\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Manifest Files Found:\")\n",
    "        if manifest_files:\n",
    "            for mf in manifest_files:\n",
    "                print(f\"   {mf}\")\n",
    "        else:\n",
    "            print(\"   âŒ No manifest files found!\")\n",
    "        \n",
    "        # Show sample files\n",
    "        print(f\"\\nğŸ“ Sample Files (first 10):\")\n",
    "        for i, obj in enumerate(all_objects[:10]):\n",
    "            size_mb = obj['Size'] / 1024 / 1024\n",
    "            print(f\"   {i+1:2d}. {obj['Key']} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Check for specific splits\n",
    "        splits = ['train', 'validation', 'test']\n",
    "        print(f\"\\nğŸ¯ Split Analysis:\")\n",
    "        for split in splits:\n",
    "            split_files = [obj for obj in all_objects if f'/{split}/' in obj['Key'] or f'_{split}_' in obj['Key']]\n",
    "            audio_files = [obj for obj in split_files if obj['Key'].endswith('.wav')]\n",
    "            manifest_files_split = [obj for obj in split_files if 'manifest' in obj['Key']]\n",
    "            \n",
    "            print(f\"   {split.capitalize()}:\")\n",
    "            print(f\"     Audio files: {len(audio_files):,}\")\n",
    "            print(f\"     Manifest files: {len(manifest_files_split)}\")\n",
    "            \n",
    "            if manifest_files_split:\n",
    "                print(f\"     Manifest path: {manifest_files_split[0]['Key']}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error exploring S3: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'total_objects': len(all_objects),\n",
    "        'folders': folders,\n",
    "        'file_types': file_types,\n",
    "        'manifest_files': manifest_files\n",
    "    }\n",
    "\n",
    "# Run the exploration\n",
    "bucket_info = explore_s3_bucket()\n",
    "\n",
    "print(\"\\nğŸ’¡ Next Steps:\")\n",
    "print(\"1. Check if manifest files exist at the expected paths\")\n",
    "print(\"2. If no manifests found, we'll need to create them or use a different loading approach\")\n",
    "print(\"3. The dataset loader will fallback to dummy data if needed for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8ca7f1-18bd-45f7-97a1-c1cb2067c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing S3 CSV Dataset Loader...\n",
      "ğŸš€ Loading Dataset from S3 CSV Manifests\n",
      "==================================================\n",
      "ğŸ” Checking for uploaded CSV manifest files...\n",
      "âœ… train_manifest.csv: 20.76 MB\n",
      "âœ… validation_manifest.csv: 2.39 MB\n",
      "âœ… test_manifest.csv: 2.40 MB\n",
      "\n",
      "ğŸ“Š Processing train split...\n",
      "ğŸ“¥ Loading train manifest from S3...\n",
      "âœ… Loaded 90606 entries\n",
      "ğŸ“Š Duration filter: 90606 â†’ 90040 samples\n",
      "ğŸ”§ Limited to 1000 samples for SageMaker efficiency\n",
      "âš ï¸ Audio casting warning: unhashable type: 'dict'\n",
      "\n",
      "ğŸ“Š Processing validation split...\n",
      "ğŸ“¥ Loading validation manifest from S3...\n",
      "âœ… Loaded 9973 entries\n",
      "ğŸ“Š Duration filter: 9973 â†’ 9917 samples\n",
      "ğŸ”§ Limited to 1000 samples for SageMaker efficiency\n",
      "âš ï¸ Audio casting warning: unhashable type: 'dict'\n",
      "\n",
      "ğŸ“Š Processing test split...\n",
      "ğŸ“¥ Loading test manifest from S3...\n",
      "âœ… Loaded 9976 entries\n",
      "ğŸ“Š Duration filter: 9976 â†’ 9916 samples\n",
      "ğŸ”§ Limited to 1000 samples for SageMaker efficiency\n",
      "âš ï¸ Audio casting warning: unhashable type: 'dict'\n",
      "\n",
      "ğŸ“Š Final Dataset Summary:\n",
      "========================================\n",
      "  Train: 1,000 samples (1.37 hours)\n",
      "  Validation: 1,000 samples (1.36 hours)\n",
      "  Test: 1,000 samples (1.19 hours)\n",
      "  Total: 3,000 samples\n",
      "\n",
      "âœ… Dataset variable 'datasets' created successfully!\n",
      "ğŸ¯ You can now proceed with training!\n",
      "\n",
      "ğŸ” Sample from train:\n",
      "   Text: á‡á˜áŸ’ášá¶á”áŸá½ášá›áŸ„á€á›áŸ„á€áŸáŸ’ášá¸á¢áŸ’á“á€á“á¶á„á€á‰áŸ’á‰á¶áŸá¼á˜áŸáŸ’áœá¶á‚á˜á“áŸá›áŸ„á€á¢áŸ’á“á€á“á¶á„á˜á€á€á¶á“áŸ‹á€á¶áš...\n",
      "   Duration: 5.0 seconds\n",
      "   Audio path: s3://pan-sea-khmer-speech-dataset-sg/khmer-whisper-dataset/d...\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# âœ… Load Dataset from S3 CSV Manifests (UPDATED)\n",
    "import json\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "class S3CSVDatasetLoader:\n",
    "    \"\"\"Load dataset from uploaded CSV manifest files in S3\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "        self.tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"km\", task=\"transcribe\")\n",
    "        self.s3_client = boto3.client('s3', region_name='ap-southeast-1')\n",
    "        self.bucket_name = 'pan-sea-khmer-speech-dataset-sg'\n",
    "        \n",
    "    def check_csv_manifests(self):\n",
    "        \"\"\"Check if CSV manifest files exist in S3\"\"\"\n",
    "        print(\"ğŸ” Checking for uploaded CSV manifest files...\")\n",
    "        \n",
    "        csv_files = {\n",
    "            'train': 'khmer-whisper-dataset/data/train/train_manifest.csv',\n",
    "            'validation': 'khmer-whisper-dataset/data/validation/validation_manifest.csv',\n",
    "            'test': 'khmer-whisper-dataset/data/test/test_manifest.csv'\n",
    "        }\n",
    "        \n",
    "        found_files = {}\n",
    "        \n",
    "        for split, s3_key in csv_files.items():\n",
    "            try:\n",
    "                response = self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)\n",
    "                size_mb = response['ContentLength'] / 1024 / 1024\n",
    "                print(f\"âœ… {split}_manifest.csv: {size_mb:.2f} MB\")\n",
    "                found_files[split] = s3_key\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {split}_manifest.csv: Not found\")\n",
    "        \n",
    "        return found_files\n",
    "    \n",
    "    def load_csv_from_s3(self, split, s3_key):\n",
    "        \"\"\"Download and load CSV manifest from S3\"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ“¥ Loading {split} manifest from S3...\")\n",
    "            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)\n",
    "            csv_content = response['Body'].read()\n",
    "            \n",
    "            # Read CSV into pandas\n",
    "            df = pd.read_csv(io.BytesIO(csv_content))\n",
    "            print(f\"âœ… Loaded {len(df)} entries\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load {split} CSV: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_datasets(self, max_duration=20.0, min_duration=1.0, max_samples=1000):\n",
    "        \"\"\"Load datasets from S3 CSV manifests\"\"\"\n",
    "        print(\"ğŸš€ Loading Dataset from S3 CSV Manifests\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Check which CSV files exist\n",
    "        csv_files = self.check_csv_manifests()\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(\"âŒ No CSV manifest files found!\")\n",
    "            print(\"ğŸ’¡ Make sure you uploaded the CSV files correctly\")\n",
    "            return self.create_dummy_datasets()\n",
    "        \n",
    "        dataset_dict = {}\n",
    "        \n",
    "        for split in ['train', 'validation', 'test']:\n",
    "            if split not in csv_files:\n",
    "                print(f\"âš ï¸ Skipping {split} - no CSV found\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nğŸ“Š Processing {split} split...\")\n",
    "            \n",
    "            # Load CSV from S3\n",
    "            df = self.load_csv_from_s3(split, csv_files[split])\n",
    "            if df is None:\n",
    "                continue\n",
    "            \n",
    "            # Filter by duration\n",
    "            original_count = len(df)\n",
    "            df_filtered = df[\n",
    "                (df['duration'] <= max_duration)\n",
    "            ]\n",
    "            print(f\"ğŸ“Š Duration filter: {original_count} â†’ {len(df_filtered)} samples\")\n",
    "            \n",
    "            # Limit samples for memory efficiency\n",
    "            if len(df_filtered) > max_samples:\n",
    "                df_filtered = df_filtered.head(max_samples)\n",
    "                print(f\"ğŸ”§ Limited to {max_samples} samples for SageMaker efficiency\")\n",
    "            \n",
    "            # Convert to dataset format\n",
    "            data = []\n",
    "            for _, row in df_filtered.iterrows():\n",
    "                # Create S3 audio path\n",
    "                audio_filename = row['audio_filepath']\n",
    "                s3_audio_path = f\"s3://{self.bucket_name}/khmer-whisper-dataset/data/{split}/audio/{audio_filename}\"\n",
    "                \n",
    "                entry = {\n",
    "                    'audio_filepath': s3_audio_path,\n",
    "                    'text': str(row['text']),\n",
    "                    'duration': float(row['duration']),\n",
    "                    'language': row.get('language', 'km'),\n",
    "                    'source': row.get('source', 'mega_dataset'),\n",
    "                    'speaker': row.get('speaker', 'unknown')\n",
    "                }\n",
    "                data.append(entry)\n",
    "            \n",
    "            # Create HuggingFace dataset\n",
    "            if data:\n",
    "                dataset = Dataset.from_list(data)\n",
    "                try:\n",
    "                    dataset = dataset.cast_column(sample['audio_filepath'], Audio(sampling_rate=16000))\n",
    "                    print(f\"âœ… Created dataset with {len(data)} samples\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Audio casting warning: {e}\")\n",
    "                \n",
    "                dataset_dict[split] = dataset\n",
    "            else:\n",
    "                print(f\"âŒ No valid data for {split}\")\n",
    "        \n",
    "        return DatasetDict(dataset_dict) if dataset_dict else self.create_dummy_datasets()\n",
    "    \n",
    "    def create_dummy_datasets(self):\n",
    "        \"\"\"Create small dummy datasets for testing\"\"\"\n",
    "        print(\"ğŸ¯ Creating dummy datasets for testing...\")\n",
    "        \n",
    "        dataset_dict = {}\n",
    "        sizes = {'train': 100, 'validation': 30, 'test': 30}\n",
    "        \n",
    "        for split, size in sizes.items():\n",
    "            data = []\n",
    "            for i in range(size):\n",
    "                data.append({\n",
    "                    'text': f'áŸá¶á€á›áŸ’á”á„ á‘á¸ {i+1}',  # \"Test number i\" in Khmer\n",
    "                    'audio_filepath': f'dummy_audio_{split}_{i}.wav',\n",
    "                    'duration': 2.0 + (i % 3),\n",
    "                    'language': 'km',\n",
    "                    'source': 'dummy',\n",
    "                    'speaker': 'unknown'\n",
    "                })\n",
    "            \n",
    "            dataset = Dataset.from_list(data)\n",
    "            dataset_dict[split] = dataset\n",
    "            print(f\"ğŸ“ {split}: {size} dummy samples\")\n",
    "        \n",
    "        return DatasetDict(dataset_dict)\n",
    "\n",
    "# Initialize loader and load datasets\n",
    "print(\"ğŸš€ Initializing S3 CSV Dataset Loader...\")\n",
    "dataset_loader = S3CSVDatasetLoader()\n",
    "\n",
    "# Load with SageMaker-friendly settings\n",
    "datasets = dataset_loader.load_datasets(\n",
    "    max_duration=20.0,\n",
    "    min_duration=1.0, \n",
    "    max_samples=1000  # Start with 1K samples per split for testing\n",
    ")\n",
    "\n",
    "# Show results\n",
    "if datasets:\n",
    "    print(f\"\\nğŸ“Š Final Dataset Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    total_samples = 0\n",
    "    for split, dataset in datasets.items():\n",
    "        count = len(dataset)\n",
    "        total_samples += count\n",
    "        \n",
    "        if 'duration' in dataset.column_names:\n",
    "            hours = sum(dataset['duration']) / 3600\n",
    "            print(f\"  {split.capitalize()}: {count:,} samples ({hours:.2f} hours)\")\n",
    "        else:\n",
    "            print(f\"  {split.capitalize()}: {count:,} samples\")\n",
    "    \n",
    "    print(f\"  Total: {total_samples:,} samples\")\n",
    "    print(f\"\\nâœ… Dataset variable 'datasets' created successfully!\")\n",
    "    print(f\"ğŸ¯ You can now proceed with training!\")\n",
    "    \n",
    "    # Show sample\n",
    "    if total_samples > 0:\n",
    "        sample_split = list(datasets.keys())[0]\n",
    "        sample = datasets[sample_split][0]\n",
    "        print(f\"\\nğŸ” Sample from {sample_split}:\")\n",
    "        print(f\"   Text: {sample['text'][:100]}...\")\n",
    "        print(f\"   Duration: {sample['duration']} seconds\")\n",
    "        print(f\"   Audio path: {sample['audio_filepath'][:60]}...\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\nâŒ Dataset loading failed!\")\n",
    "    print(f\"ğŸ’¡ Check CSV uploads and try again\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e42ccb-9739-41ff-9dfb-f572640f8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Setting up Whisper models for training...\n",
      "ğŸ”§ Loading Whisper TINY components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Whisper TINY loaded successfully!\n",
      "   Parameters: 37,760,640\n",
      "   Trainable: 37,184,640\n",
      "\n",
      "ğŸ“Š Model Configurations:\n",
      "  Whisper Tiny - Batch Size: 16\n",
      "  Whisper Base - Batch Size: 8\n",
      "  Max Length: 448\n",
      "  Learning Rate: 1e-05\n",
      "  Gradient Accumulation: 2\n",
      "\n",
      "ğŸ’¾ GPU Memory: 14.6 GB\n",
      "âœ… Configurations optimized for ml.g4dn.2xlarge (16GB VRAM)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    WhisperForConditionalGeneration, \n",
    "    WhisperTokenizer, \n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperProcessor\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "\n",
    "@dataclass\n",
    "class WhisperModelConfig:\n",
    "    \"\"\"Configuration for Whisper models optimized for ml.g4dn.2xlarge\"\"\"\n",
    "    model_name: str\n",
    "    max_length: int = 448\n",
    "    language: str = \"km\"\n",
    "    task: str = \"transcribe\"\n",
    "    batch_size_tiny: int = 16\n",
    "    batch_size_base: int = 8\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    learning_rate: float = 1e-5\n",
    "    warmup_steps: int = 500\n",
    "    max_steps: int = 5000\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 1000\n",
    "\n",
    "class WhisperModelManager:\n",
    "    \"\"\"Manage Whisper model configurations and initialization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'tiny': WhisperModelConfig(\"openai/whisper-tiny\"),\n",
    "            'base': WhisperModelConfig(\"openai/whisper-base\")\n",
    "        }\n",
    "    \n",
    "    def load_model_components(self, model_size):\n",
    "        \"\"\"Load tokenizer, feature extractor, and model\"\"\"\n",
    "        config = self.models[model_size]\n",
    "        \n",
    "        print(f\"ğŸ”§ Loading Whisper {model_size.upper()} components...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = WhisperTokenizer.from_pretrained(\n",
    "            config.model_name, \n",
    "            language=config.language, \n",
    "            task=config.task\n",
    "        )\n",
    "        \n",
    "        # Load feature extractor\n",
    "        feature_extractor = WhisperFeatureExtractor.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Load processor (combines tokenizer and feature extractor)\n",
    "        processor = WhisperProcessor.from_pretrained(config.model_name)\n",
    "        processor.tokenizer = tokenizer\n",
    "        \n",
    "        # Load model\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Configure model for Khmer\n",
    "        model.generation_config.language = config.language\n",
    "        model.generation_config.task = config.task\n",
    "        model.generation_config.forced_decoder_ids = None\n",
    "        \n",
    "        print(f\"âœ… Whisper {model_size.upper()} loaded successfully!\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"   Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "        return tokenizer, feature_extractor, processor, model, config\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = WhisperModelManager()\n",
    "\n",
    "# Load Whisper Tiny components\n",
    "print(\"ğŸš€ Setting up Whisper models for training...\")\n",
    "tiny_tokenizer, tiny_feature_extractor, tiny_processor, tiny_model, tiny_config = model_manager.load_model_components('tiny')\n",
    "\n",
    "print(f\"\\nğŸ“Š Model Configurations:\")\n",
    "print(f\"  Whisper Tiny - Batch Size: {tiny_config.batch_size_tiny}\")\n",
    "print(f\"  Whisper Base - Batch Size: {tiny_config.batch_size_base}\")\n",
    "print(f\"  Max Length: {tiny_config.max_length}\")\n",
    "print(f\"  Learning Rate: {tiny_config.learning_rate}\")\n",
    "print(f\"  Gradient Accumulation: {tiny_config.gradient_accumulation_steps}\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\\nğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(\"âœ… Configurations optimized for ml.g4dn.2xlarge (16GB VRAM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866debff-213b-41a6-97ae-a7700ecd654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training configuration setup complete!\n",
      "ğŸ¯ Optimized for ml.g4dn.2xlarge with 16GB VRAM\n",
      "âœ… Mixed precision (FP16) enabled\n",
      "âœ… Gradient checkpointing enabled for memory efficiency\n",
      "âœ… Early stopping configured\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"Data collator for speech-to-text training\"\"\"\n",
    "    \n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        model_input_name = self.processor.model_input_names[0]\n",
    "        input_features = [{model_input_name: feature[model_input_name]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "def create_training_arguments(model_size, config, output_dir):\n",
    "    \"\"\"Create optimized training arguments for ml.g4dn.2xlarge - Compatible Version\"\"\"\n",
    "    \n",
    "    # Adjust batch size based on model size\n",
    "    if model_size == 'tiny':\n",
    "        per_device_train_batch_size = config.batch_size_tiny\n",
    "        per_device_eval_batch_size = config.batch_size_tiny\n",
    "    else:  # base\n",
    "        per_device_train_batch_size = config.batch_size_base\n",
    "        per_device_eval_batch_size = config.batch_size_base\n",
    "    \n",
    "    # Compatible training arguments (removed newer parameters)\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        max_steps=config.max_steps,\n",
    "        gradient_checkpointing=True,  # Save memory\n",
    "        fp16=True,  # Mixed precision for faster training\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_steps=config.save_steps,\n",
    "        logging_steps=100,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=3,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=config.max_length,\n",
    "        # Removed dispatch_batches and other newer parameters\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"]\n",
    "    )\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"Compute WER and CER metrics\"\"\"\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    cer_metric = evaluate.load(\"cer\")\n",
    "    \n",
    "    pred_ids, label_ids = eval_preds\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute metrics\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "def setup_trainer(model, tokenizer, processor, config, datasets, model_size):\n",
    "    \"\"\"Setup Seq2SeqTrainer with all components - Compatible Version\"\"\"\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # Create training arguments\n",
    "    output_dir = f\"./whisper-{model_size}-khmer\"\n",
    "    training_args = create_training_arguments(model_size, config, output_dir)\n",
    "    \n",
    "    # Create trainer with compatible parameters\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_preds: compute_metrics(eval_preds, tokenizer),\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    return trainer, training_args\n",
    "\n",
    "print(\"âš™ï¸ Training configuration setup complete! (Compatible Version)\")\n",
    "print(\"ğŸ¯ Optimized for ml.g4dn.2xlarge with 16GB VRAM\")\n",
    "print(\"âœ… Mixed precision (FP16) enabled\")\n",
    "print(\"âœ… Gradient checkpointing enabled for memory efficiency\")\n",
    "print(\"âœ… Early stopping configured\")\n",
    "print(\"ğŸ”§ Compatible with older accelerate versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da2dcb8-3ba3-4067-a29e-71e0ed791d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking Prerequisites for Whisper Training...\n",
      "==================================================\n",
      "ğŸ¯ Available Variables:\n",
      "   âœ… datasets - Dataset loading (Cell 7)\n",
      "   âœ… tiny_model - Whisper Tiny model (Cell 9)\n",
      "   âœ… tiny_tokenizer - Whisper Tiny tokenizer (Cell 9)\n",
      "   âœ… tiny_processor - Whisper Tiny processor (Cell 9)\n",
      "   âœ… tiny_config - Whisper Tiny config (Cell 9)\n",
      "   âœ… setup_trainer - Training setup function (Cell 11)\n",
      "\n",
      "ğŸ‰ All prerequisites available!\n",
      "âœ… Ready to start Whisper training\n",
      "\n",
      "ğŸ“Š Dataset Info:\n",
      "   train: 1,000 samples\n",
      "   validation: 1,000 samples\n",
      "   test: 1,000 samples\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# âœ… Pre-Training Checklist - Run This Before Training\n",
    "print(\"ğŸ” Checking Prerequisites for Whisper Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if all required variables are defined\n",
    "required_vars = {\n",
    "    'datasets': 'Dataset loading (Cell 7)',\n",
    "    'tiny_model': 'Whisper Tiny model (Cell 9)', \n",
    "    'tiny_tokenizer': 'Whisper Tiny tokenizer (Cell 9)',\n",
    "    'tiny_processor': 'Whisper Tiny processor (Cell 9)',\n",
    "    'tiny_config': 'Whisper Tiny config (Cell 9)',\n",
    "    'setup_trainer': 'Training setup function (Cell 11)'\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "available_vars = []\n",
    "\n",
    "for var_name, description in required_vars.items():\n",
    "    try:\n",
    "        if var_name in locals() or var_name in globals():\n",
    "            available_vars.append(f\"âœ… {var_name} - {description}\")\n",
    "        else:\n",
    "            missing_vars.append(f\"âŒ {var_name} - {description}\")\n",
    "    except:\n",
    "        missing_vars.append(f\"âŒ {var_name} - {description}\")\n",
    "\n",
    "# Show status\n",
    "print(\"ğŸ¯ Available Variables:\")\n",
    "for var in available_vars:\n",
    "    print(f\"   {var}\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\nâš ï¸ Missing Variables ({len(missing_vars)}):\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   {var}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ Before training, you need to run these cells in order:\")\n",
    "    print(f\"   1. Cell 3: SageMaker Setup\")\n",
    "    print(f\"   2. Cell 5: Package Installation\") \n",
    "    print(f\"   3. Cell 7: Dataset Loading\")\n",
    "    print(f\"   4. Cell 9: Model Configuration\")\n",
    "    print(f\"   5. Cell 11: Training Setup\")\n",
    "    print(f\"   6. Then you can run the training cells\")\n",
    "    \n",
    "    print(f\"\\nğŸš« Cannot start training - missing prerequisites!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nğŸ‰ All prerequisites available!\")\n",
    "    print(f\"âœ… Ready to start Whisper training\")\n",
    "    \n",
    "    # Show some stats if datasets is available\n",
    "    try:\n",
    "        if 'datasets' in locals() or 'datasets' in globals():\n",
    "            print(f\"\\nğŸ“Š Dataset Info:\")\n",
    "            for split in datasets.keys():\n",
    "                print(f\"   {split}: {len(datasets[split]):,} samples\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b7f8e0-2e26-44a0-a5b7-641e65dbbc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Whisper Tiny Fine-tuning...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in &lt;module&gt;:9                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"=\"</span> * <span style=\"color: #0000ff; text-decoration-color: #0000ff\">60</span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 # Setup trainer for Tiny model</span>                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 9 tiny_trainer, tiny_training_args = setup_trainer(                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>model=tiny_model,                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>tokenizer=tiny_tokenizer,                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>processor=tiny_processor,                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in setup_trainer:108                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>training_args = create_training_arguments(model_size, config, output_dir)              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Create trainer</span>                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>108 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>trainer = Seq2SeqTrainer(                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>args=training_args,                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>model=model,                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>train_dataset=datasets[<span style=\"color: #808000; text-decoration-color: #808000\">\"train\"</span>],                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/</span><span style=\"font-weight: bold\">trainer_seq2seq.</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"font-weight: bold\">py</span>:56 in __init__                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 53 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">N</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 54 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], t   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 55 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>):                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 56 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 57 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>model=model,                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 58 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>args=args,                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 59 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>data_collator=data_collator,                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/</span><span style=\"font-weight: bold\">trainer.py</span>:342   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in __init__                                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 339 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.is_in_train = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 341 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 342 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.create_accelerator_and_postprocess()                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 343 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 344 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># memory metrics - must set up as early as possible</span>                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 345 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._memory_tracker = TrainerMemoryTracker(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.skip_memory_metrics)        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/</span><span style=\"font-weight: bold\">trainer.py</span>:3925  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in create_accelerator_and_postprocess                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3922 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>gradient_accumulation_plugin = GradientAccumulationPlugin(**grad_acc_kwargs)      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3923 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3924 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># create accelerator object</span>                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>3925 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator = Accelerator(                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3926 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>dispatch_batches=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.dispatch_batches,                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3927 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>split_batches=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.split_batches,                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3928 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>deepspeed_plugin=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.deepspeed_plugin,                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #e100e1; text-decoration-color: #e100e1; font-weight: bold\">Accelerator.__init__</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008700; text-decoration-color: #008700\">'dispatch_batches'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0mâ•­â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in <module>:9                                                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"\u001b[0m * \u001b[94m60\u001b[0m)                                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 7 \u001b[0m                                                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m# Setup trainer for Tiny model\u001b[0m                                                              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 9 tiny_trainer, tiny_training_args = setup_trainer(                                           \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2mâ”‚   \u001b[0mmodel=tiny_model,                                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2mâ”‚   \u001b[0mtokenizer=tiny_tokenizer,                                                               \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2mâ”‚   \u001b[0mprocessor=tiny_processor,                                                               \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in setup_trainer:108                                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2mâ”‚   \u001b[0mtraining_args = create_training_arguments(model_size, config, output_dir)              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[2m# Create trainer\u001b[0m                                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m108 \u001b[2mâ”‚   \u001b[0mtrainer = Seq2SeqTrainer(                                                              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0margs=training_args,                                                                \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mmodel=model,                                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mtrain_dataset=datasets[\u001b[33m\"\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m\"\u001b[0m],                                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1mtrainer_seq2seq.\u001b[0m \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[1mpy\u001b[0m:56 in __init__                                                                                \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 53 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0moptimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (\u001b[94mN\u001b[0m   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 54 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mpreprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], t   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 55 \u001b[0m\u001b[2mâ”‚   \u001b[0m):                                                                                     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 56 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 57 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mmodel=model,                                                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 58 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0margs=args,                                                                     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 59 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mdata_collator=data_collator,                                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1mtrainer.py\u001b[0m:342   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in __init__                                                                                      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 339 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.deepspeed = \u001b[94mNone\u001b[0m                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 340 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.is_in_train = \u001b[94mFalse\u001b[0m                                                          \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 341 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 342 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.create_accelerator_and_postprocess()                                         \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 343 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 344 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[2m# memory metrics - must set up as early as possible\u001b[0m                               \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 345 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m._memory_tracker = TrainerMemoryTracker(\u001b[96mself\u001b[0m.args.skip_memory_metrics)        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1mtrainer.py\u001b[0m:3925  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in create_accelerator_and_postprocess                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m3922 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mgradient_accumulation_plugin = GradientAccumulationPlugin(**grad_acc_kwargs)      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m3923 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m3924 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[2m# create accelerator object\u001b[0m                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m3925 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.accelerator = Accelerator(                                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m3926 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mdispatch_batches=\u001b[96mself\u001b[0m.args.dispatch_batches,                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m3927 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0msplit_batches=\u001b[96mself\u001b[0m.args.split_batches,                                        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m3928 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mdeepspeed_plugin=\u001b[96mself\u001b[0m.args.deepspeed_plugin,                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;38;2;225;0;225mAccelerator.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[38;2;0;135;0m'dispatch_batches'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"ğŸš€ Starting Whisper Tiny Fine-tuning...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Setup trainer for Tiny model\n",
    "tiny_trainer, tiny_training_args = setup_trainer(\n",
    "    model=tiny_model,\n",
    "    tokenizer=tiny_tokenizer, \n",
    "    processor=tiny_processor,\n",
    "    config=tiny_config,\n",
    "    datasets=datasets,\n",
    "    model_size=\"tiny\"\n",
    ")\n",
    "\n",
    "# Print training info\n",
    "print(f\"ğŸ“Š Whisper Tiny Training Configuration:\")\n",
    "print(f\"   Model Parameters: {sum(p.numel() for p in tiny_model.parameters()):,}\")\n",
    "print(f\"   Batch Size: {tiny_training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient Accumulation: {tiny_training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective Batch Size: {tiny_training_args.per_device_train_batch_size * tiny_training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Max Steps: {tiny_training_args.max_steps}\")\n",
    "print(f\"   Learning Rate: {tiny_training_args.learning_rate}\")\n",
    "print(f\"   Output Directory: {tiny_training_args.output_dir}\")\n",
    "\n",
    "# Check GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\nğŸ’¾ Pre-training GPU Memory:\")\n",
    "    print(f\"   Allocated: {allocated_memory:.2f} GB\")\n",
    "    print(f\"   Cached: {cached_memory:.2f} GB\")\n",
    "\n",
    "print(f\"\\nâ° Training Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ğŸ¯ Expected training time: ~3-4 hours\")\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    tiny_trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nğŸ‰ Whisper Tiny Training Complete!\")\n",
    "    print(f\"â±ï¸ Total Training Time: {training_time/3600:.2f} hours\")\n",
    "    \n",
    "    # Save the final model\n",
    "    tiny_trainer.save_model()\n",
    "    tiny_trainer.tokenizer.save_pretrained(tiny_training_args.output_dir)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Model saved to: {tiny_training_args.output_dir}\")\n",
    "    \n",
    "    # Get final metrics\n",
    "    final_logs = tiny_trainer.state.log_history[-1]\n",
    "    if 'eval_wer' in final_logs:\n",
    "        print(f\"ğŸ“Š Final WER: {final_logs['eval_wer']:.4f}\")\n",
    "        print(f\"ğŸ“Š Final CER: {final_logs.get('eval_cer', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Memory usage after training\n",
    "    if torch.cuda.is_available():\n",
    "        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        max_memory = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "        print(f\"\\nğŸ’¾ Post-training GPU Memory:\")\n",
    "        print(f\"   Current: {allocated_memory:.2f} GB\")\n",
    "        print(f\"   Peak: {max_memory:.2f} GB\")\n",
    "        \n",
    "        # Clear cache for next model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"ğŸ§¹ GPU cache cleared for next model\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0d7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb65371-5f1d-42f1-a7ba-f9e620e50944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
