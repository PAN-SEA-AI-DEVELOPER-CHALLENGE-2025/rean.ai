{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8680912d",
   "metadata": {},
   "source": [
    "## ⚡ QUICK START - Execute These Cells in Order\n",
    "\n",
    "**IMPORTANT: You must run these cells sequentially before training:**\n",
    "\n",
    "1. ✅ **Cell 6** - Load Dataset from S3 (creates `datasets` variable)\n",
    "2. ✅ **Cell 9** - Model Configuration (creates `tiny_feature_extractor`, `tiny_tokenizer`)  \n",
    "3. ✅ **Cell 8** - Preprocess Dataset (converts audio to training format)\n",
    "4. ✅ **Cell 10** - Training Setup\n",
    "5. ✅ **Cell 11** - Pre-Training Checklist\n",
    "\n",
    "**Current Status:**\n",
    "- ❌ `tiny_feature_extractor` missing → **RUN CELL 9 FIRST**\n",
    "- ❌ `tiny_tokenizer` missing → **RUN CELL 9 FIRST**\n",
    "\n",
    "**After Cell 9, Cell 8 (preprocessing) will work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84ae99",
   "metadata": {},
   "source": [
    "# 🚀 Whisper Fine-tuning on SageMaker ml.g4dn.2xlarge\n",
    "\n",
    "## Khmer Speech Recognition with Mega Dataset\n",
    "\n",
    "This notebook fine-tunes Whisper Tiny and Base models on Amazon SageMaker using the comprehensive Mega Khmer dataset (160+ hours, 110K+ samples) on ml.g4dn.2xlarge instances.\n",
    "\n",
    "### 🎯 What We'll Accomplish:\n",
    "- ✅ Fine-tune Whisper Tiny (~39M params)\n",
    "- ✅ Fine-tune Whisper Base (~74M params)  \n",
    "- ✅ GPU-optimized for ml.g4dn.2xlarge (16GB VRAM)\n",
    "- ✅ Production-ready model deployment\n",
    "\n",
    "### 📊 Dataset Overview:\n",
    "- **Total Duration**: 160+ hours\n",
    "- **Samples**: 110,000+\n",
    "- **Sources**: Original + LSR42 + Rinabuoy datasets\n",
    "- **Language**: Khmer (km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ab34a",
   "metadata": {},
   "source": [
    "## 🚀 SageMaker Execution Guide\n",
    "\n",
    "### 📋 **IMPORTANT: Run Cells in This Order**\n",
    "\n",
    "**Before Training, Execute These Cells Sequentially:**\n",
    "\n",
    "1. **Cell 3**: 🔧 SageMaker Environment Setup \n",
    "2. **Cell 5**: 📦 Package Installation\n",
    "3. **Cell 6**: 📊 Dataset Loading from S3  \n",
    "4. **Cell 8**: 🔄 Dataset Preprocessing for Training\n",
    "5. **Cell 9**: 🤖 Model Configuration (Tiny & Base)\n",
    "6. **Cell 10**: ⚙️ Training Arguments Setup\n",
    "7. **Cell 11**: ✅ Pre-Training Checklist (verify all variables)\n",
    "\n",
    "**Then Choose Your Training:**\n",
    "- **Cell 12**: 🎯 Train Whisper Tiny (~3-4 hours)\n",
    "- **Cell 14**: 🎯 Train Whisper Base (~5-6 hours)\n",
    "\n",
    "### ⚠️ **Common Issues:**\n",
    "- **`NameError: name 'datasets' is not defined`** → Run cells 3, 5, 6 first\n",
    "- **Package installation errors** → Use conda commands in cell 5\n",
    "- **S3 access errors** → Verify your bucket name in cell 3\n",
    "\n",
    "### 💡 **Tips:**\n",
    "- This notebook is designed for **ml.g4dn.2xlarge** instances\n",
    "- Training uses your uploaded S3 dataset: `s3://pan-sea-khmer-speech-dataset-sg/`\n",
    "- Both models will be saved locally and can be uploaded to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f1c13",
   "metadata": {},
   "source": [
    "## 1. Setup SageMaker Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7e432",
   "metadata": {},
   "source": [
    "## 2. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba00299",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Mega Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb0390d-0802-48db-8463-f3e40f79665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing S3 CSV Dataset Loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading Dataset from S3 CSV Manifests\n",
      "==================================================\n",
      "🔍 Checking for uploaded CSV manifest files...\n",
      "✅ train_manifest.csv: 20.76 MB\n",
      "✅ validation_manifest.csv: 2.39 MB\n",
      "✅ test_manifest.csv: 2.40 MB\n",
      "\n",
      "📊 Processing train split...\n",
      "📥 Loading train manifest from S3...\n",
      "✅ Loaded 90606 entries\n",
      "📋 CSV structure for train:\n",
      "   Sample audio_filepath: audio/c01b83b9-c71b-45e5-b68f-9b9e86a29cab_1_0.wav\n",
      "   Available columns: ['audio_filepath', 'text', 'duration', 'language', 'speaker', 'session_id', 'source']\n",
      "📊 Duration filter (1.0-20.0s): 90,606 → 89,034 samples\n",
      "🎯 Using ALL 89,034 samples from train\n",
      "✅ Created dataset with 89034 samples\n",
      "\n",
      "📊 Processing validation split...\n",
      "📥 Loading validation manifest from S3...\n",
      "✅ Loaded 9973 entries\n",
      "📋 CSV structure for validation:\n",
      "   Sample audio_filepath: audio/88bda0d0-c935-44c6-89ba-c71f40f5d02b_0_1.wav\n",
      "   Available columns: ['audio_filepath', 'text', 'duration', 'language', 'speaker', 'session_id', 'source']\n",
      "📊 Duration filter (1.0-20.0s): 9,973 → 9,917 samples\n",
      "🎯 Using ALL 9,917 samples from validation\n",
      "✅ Created dataset with 9917 samples\n",
      "\n",
      "📊 Processing test split...\n",
      "📥 Loading test manifest from S3...\n",
      "✅ Loaded 9976 entries\n",
      "📋 CSV structure for test:\n",
      "   Sample audio_filepath: audio/be28e3d7-1a37-4072-a618-17020c04dd94_0.wav\n",
      "   Available columns: ['audio_filepath', 'text', 'duration', 'language', 'speaker', 'session_id', 'source']\n",
      "📊 Duration filter (1.0-20.0s): 9,976 → 9,916 samples\n",
      "🎯 Using ALL 9,916 samples from test\n",
      "✅ Created dataset with 9916 samples\n",
      "\n",
      "📊 Final Dataset Summary:\n",
      "========================================\n",
      "  Train: 89,034 samples (105.45 hours)\n",
      "  Validation: 9,917 samples (13.51 hours)\n",
      "  Test: 9,916 samples (13.03 hours)\n",
      "  Total: 108,867 samples\n",
      "\n",
      "✅ Dataset variable 'datasets' created successfully!\n",
      "🎯 You can now proceed with training!\n",
      "\n",
      "🔍 Sample from train:\n",
      "   Text: ជម្រាបសួរលោកលោកស្រីអ្នកនាងកញ្ញាសូមស្វាគមន៍លោកអ្នកនាងមកកាន់ការ...\n",
      "   Duration: 5.0 seconds\n",
      "   Audio path: {'path': 's3://pan-sea-khmer-speech-dataset-sg/khmer-whisper...\n"
     ]
    }
   ],
   "source": [
    "# ✅ Load Dataset from S3 CSV Manifests (UPDATED)\n",
    "import json\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "class S3CSVDatasetLoader:\n",
    "    \"\"\"Load dataset from uploaded CSV manifest files in S3\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "        self.tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"km\", task=\"transcribe\")\n",
    "        self.s3_client = boto3.client('s3', region_name='ap-southeast-1')\n",
    "        self.bucket_name = 'pan-sea-khmer-speech-dataset-sg'\n",
    "        \n",
    "    def check_csv_manifests(self):\n",
    "        \"\"\"Check if CSV manifest files exist in S3\"\"\"\n",
    "        print(\"🔍 Checking for uploaded CSV manifest files...\")\n",
    "        \n",
    "        csv_files = {\n",
    "            'train': 'khmer-whisper-dataset/data/train/train_manifest.csv',\n",
    "            'validation': 'khmer-whisper-dataset/data/validation/validation_manifest.csv',\n",
    "            'test': 'khmer-whisper-dataset/data/test/test_manifest.csv'\n",
    "        }\n",
    "        \n",
    "        found_files = {}\n",
    "        \n",
    "        for split, s3_key in csv_files.items():\n",
    "            try:\n",
    "                response = self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)\n",
    "                size_mb = response['ContentLength'] / 1024 / 1024\n",
    "                print(f\"✅ {split}_manifest.csv: {size_mb:.2f} MB\")\n",
    "                found_files[split] = s3_key\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {split}_manifest.csv: Not found\")\n",
    "        \n",
    "        return found_files\n",
    "    \n",
    "    def load_csv_from_s3(self, split, s3_key):\n",
    "        \"\"\"Download and load CSV manifest from S3\"\"\"\n",
    "        try:\n",
    "            print(f\"📥 Loading {split} manifest from S3...\")\n",
    "            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)\n",
    "            csv_content = response['Body'].read()\n",
    "            \n",
    "            # Read CSV into pandas\n",
    "            df = pd.read_csv(io.BytesIO(csv_content))\n",
    "            print(f\"✅ Loaded {len(df)} entries\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {split} CSV: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_datasets(self, max_duration=20.0, min_duration=1.0, max_samples=None):\n",
    "        \"\"\"Load datasets from S3 CSV manifests\"\"\"\n",
    "        print(\"🚀 Loading Dataset from S3 CSV Manifests\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Check which CSV files exist\n",
    "        csv_files = self.check_csv_manifests()\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(\"❌ No CSV manifest files found!\")\n",
    "            print(\"💡 Make sure you uploaded the CSV files correctly\")\n",
    "            return self.create_dummy_datasets()\n",
    "        \n",
    "        dataset_dict = {}\n",
    "        \n",
    "        for split in ['train', 'validation', 'test']:\n",
    "            if split not in csv_files:\n",
    "                print(f\"⚠️ Skipping {split} - no CSV found\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n📊 Processing {split} split...\")\n",
    "            \n",
    "            # Load CSV from S3\n",
    "            df = self.load_csv_from_s3(split, csv_files[split])\n",
    "            if df is None:\n",
    "                continue\n",
    "            \n",
    "            # Show sample of CSV structure for debugging\n",
    "            print(f\"📋 CSV structure for {split}:\")\n",
    "            if len(df) > 0:\n",
    "                sample_row = df.iloc[0]\n",
    "                print(f\"   Sample audio_filepath: {sample_row['audio_filepath']}\")\n",
    "                print(f\"   Available columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Filter by duration (keep reasonable lengths)\n",
    "            original_count = len(df)\n",
    "            df_filtered = df[\n",
    "                (df['duration'] >= min_duration) & \n",
    "                (df['duration'] <= max_duration)\n",
    "            ]\n",
    "            print(f\"📊 Duration filter ({min_duration}-{max_duration}s): {original_count:,} → {len(df_filtered):,} samples\")\n",
    "            \n",
    "            # Limit samples for memory efficiency\n",
    "            if max_samples and len(df_filtered) > max_samples:\n",
    "                df_filtered = df_filtered.head(max_samples)\n",
    "                print(f\"🔧 Limited to {max_samples} samples for testing\")\n",
    "            else:\n",
    "                print(f\"🎯 Using ALL {len(df_filtered):,} samples from {split}\")\n",
    "            \n",
    "            # Convert to dataset format with corrected S3 paths\n",
    "            data = []\n",
    "            for _, row in df_filtered.iterrows():\n",
    "                # Create S3 audio path - remove double audio/ paths\n",
    "                audio_filename = row['audio_filepath']\n",
    "                \n",
    "                # Clean up the audio filename - remove duplicate audio/ paths\n",
    "                if audio_filename.startswith('audio/audio/'):\n",
    "                    # Remove the first audio/ to fix duplication\n",
    "                    audio_filename = audio_filename[6:]  # Remove first 'audio/'\n",
    "                elif audio_filename.startswith('audio/'):\n",
    "                    # Keep as is\n",
    "                    pass\n",
    "                else:\n",
    "                    # Add audio/ prefix if missing\n",
    "                    audio_filename = f\"audio/{audio_filename}\"\n",
    "                \n",
    "                s3_audio_path = f\"s3://{self.bucket_name}/khmer-whisper-dataset/data/{split}/{audio_filename}\"\n",
    "                \n",
    "                entry = {\n",
    "                    'audio_filepath': s3_audio_path,\n",
    "                    'text': str(row['text']),\n",
    "                    'duration': float(row['duration']),\n",
    "                    'language': row.get('language', 'km'),\n",
    "                    'source': row.get('source', 'mega_dataset'),\n",
    "                    'speaker': row.get('speaker', 'unknown')\n",
    "                }\n",
    "                data.append(entry)\n",
    "            \n",
    "            # Create HuggingFace dataset\n",
    "            if data:\n",
    "                dataset = Dataset.from_list(data)\n",
    "                try:\n",
    "                    dataset = dataset.cast_column(\"audio_filepath\", Audio(sampling_rate=16000))\n",
    "                    print(f\"✅ Created dataset with {len(data)} samples\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Audio casting warning: {e}\")\n",
    "                \n",
    "                dataset_dict[split] = dataset\n",
    "            else:\n",
    "                print(f\"❌ No valid data for {split}\")\n",
    "        \n",
    "        return DatasetDict(dataset_dict) if dataset_dict else self.create_dummy_datasets()\n",
    "    \n",
    "    def create_dummy_datasets(self):\n",
    "        \"\"\"Create small dummy datasets for testing\"\"\"\n",
    "        print(\"🎯 Creating dummy datasets for testing...\")\n",
    "        \n",
    "        dataset_dict = {}\n",
    "        sizes = {'train': 100, 'validation': 30, 'test': 30}\n",
    "        \n",
    "        for split, size in sizes.items():\n",
    "            data = []\n",
    "            for i in range(size):\n",
    "                data.append({\n",
    "                    'text': f'សាកល្បង ទី {i+1}',  # \"Test number i\" in Khmer\n",
    "                    'audio_filepath': f'dummy_audio_{split}_{i}.wav',\n",
    "                    'duration': 2.0 + (i % 3),\n",
    "                    'language': 'km',\n",
    "                    'source': 'dummy',\n",
    "                    'speaker': 'unknown'\n",
    "                })\n",
    "            \n",
    "            dataset = Dataset.from_list(data)\n",
    "            dataset_dict[split] = dataset\n",
    "            print(f\"📝 {split}: {size} dummy samples\")\n",
    "        \n",
    "        return DatasetDict(dataset_dict)\n",
    "\n",
    "# Initialize loader and load datasets\n",
    "print(\"🚀 Initializing S3 CSV Dataset Loader...\")\n",
    "dataset_loader = S3CSVDatasetLoader()\n",
    "\n",
    "# Load FULL dataset (all samples)\n",
    "datasets = dataset_loader.load_datasets(\n",
    "    max_duration=20.0,\n",
    "    min_duration=1.0, \n",
    "    max_samples=None  # Use ALL samples - no limit!\n",
    ")\n",
    "\n",
    "# Show results\n",
    "if datasets:\n",
    "    print(f\"\\n📊 Final Dataset Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    total_samples = 0\n",
    "    for split, dataset in datasets.items():\n",
    "        count = len(dataset)\n",
    "        total_samples += count\n",
    "        \n",
    "        if 'duration' in dataset.column_names:\n",
    "            hours = sum(dataset['duration']) / 3600\n",
    "            print(f\"  {split.capitalize()}: {count:,} samples ({hours:.2f} hours)\")\n",
    "        else:\n",
    "            print(f\"  {split.capitalize()}: {count:,} samples\")\n",
    "    \n",
    "    print(f\"  Total: {total_samples:,} samples\")\n",
    "    print(f\"\\n✅ Dataset variable 'datasets' created successfully!\")\n",
    "    print(f\"🎯 You can now proceed with training!\")\n",
    "    \n",
    "    # Show sample\n",
    "    if total_samples > 0:\n",
    "        sample_split = list(datasets.keys())[0]\n",
    "        sample = datasets[sample_split][0]\n",
    "        print(f\"\\n🔍 Sample from {sample_split}:\")\n",
    "        print(f\"   Text: {sample['text'][:100]}...\")\n",
    "        print(f\"   Duration: {sample['duration']} seconds\")\n",
    "        \n",
    "        # Handle audio path safely (could be string, list, or other type)\n",
    "        audio_path = sample['audio_filepath']\n",
    "        if isinstance(audio_path, str):\n",
    "            print(f\"   Audio path: {audio_path[:60]}...\")\n",
    "        else:\n",
    "            print(f\"   Audio path: {str(audio_path)[:60]}...\")\n",
    "        \n",
    "\n",
    "else:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"❌ Dataset loading failed!\")\n",
    "    print(\"💡 Check CSV uploads and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac42ba",
   "metadata": {},
   "source": [
    "## 4. Preprocess Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 CRITICAL: Preprocess Raw Dataset for Training\n",
    "# This converts raw datasets (audio_filepath, text) -> training format (input_features, labels)\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "def preprocess_dataset_for_training(raw_datasets, feature_extractor, tokenizer, max_samples=None):\n",
    "    \"\"\"\n",
    "    Convert raw dataset with audio_filepath/text to training format with input_features/labels\n",
    "    NO DUMMY DATA - FAIL FAST ON REAL ERRORS\n",
    "    \"\"\"\n",
    "    print(\"🔄 PREPROCESSING: Converting raw datasets to training format...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize S3 client for audio loading\n",
    "    s3_client = boto3.client('s3', region_name='ap-southeast-1')\n",
    "    \n",
    "    processed_datasets = {}\n",
    "    \n",
    "    for split_name, dataset in raw_datasets.items():\n",
    "        print(f\"\\n📊 Processing {split_name} split...\")\n",
    "        \n",
    "        # Debug: Show first example structure\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0]\n",
    "            print(f\"🔍 DEBUG - Sample structure:\")\n",
    "            for key, value in sample.items():\n",
    "                print(f\"   {key}: {type(value)} = {str(value)[:100]}...\")\n",
    "        \n",
    "        # Limit samples for memory efficiency during testing\n",
    "        if max_samples and len(dataset) > max_samples:\n",
    "            dataset = dataset.select(range(max_samples))\n",
    "            print(f\"🔧 Limited to {max_samples} samples for preprocessing test\")\n",
    "        \n",
    "        processed_examples = []\n",
    "        errors = 0\n",
    "        \n",
    "        for i, example in enumerate(dataset):\n",
    "            try:\n",
    "                # Show progress\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"   Processing {i}/{len(dataset)}...\")\n",
    "                \n",
    "                # Initialize audio_array variable\n",
    "                audio_array = None\n",
    "                sampling_rate = 16000\n",
    "                \n",
    "                # Get audio file path - handle both dict and string formats\n",
    "                audio_path = example['audio_filepath']\n",
    "                \n",
    "                # If audio_filepath is a dict (from HuggingFace Audio type), extract the path\n",
    "                if isinstance(audio_path, dict):\n",
    "                    if 'path' in audio_path:\n",
    "                        audio_path = audio_path['path']\n",
    "                    elif 'array' in audio_path:\n",
    "                        # Audio data is already loaded, use it directly\n",
    "                        audio_array = audio_path['array']\n",
    "                        sampling_rate = audio_path.get('sampling_rate', 16000)\n",
    "                        \n",
    "                        # Resample if needed\n",
    "                        if sampling_rate != 16000:\n",
    "                            audio_array = librosa.resample(audio_array, orig_sr=sampling_rate, target_sr=16000)\n",
    "                            sampling_rate = 16000\n",
    "                    else:\n",
    "                        print(f\"⚠️ Unexpected audio dict format: {list(audio_path.keys())}\")\n",
    "                        continue\n",
    "                \n",
    "                # If we don't have audio_array yet, try to load from path\n",
    "                if audio_array is None:\n",
    "                    # Handle S3 paths\n",
    "                    if isinstance(audio_path, str) and audio_path.startswith('s3://'):\n",
    "                        # Parse S3 path: s3://bucket/path/to/file.wav\n",
    "                        path_parts = audio_path[5:].split('/', 1)  # Remove 's3://'\n",
    "                        bucket_name = path_parts[0]\n",
    "                        s3_key = path_parts[1]\n",
    "                        \n",
    "                        # Download audio from S3\n",
    "                        try:\n",
    "                            response = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "                            audio_bytes = response['Body'].read()\n",
    "                            \n",
    "                            # Load audio with librosa from bytes\n",
    "                            audio_array, sampling_rate = librosa.load(\n",
    "                                BytesIO(audio_bytes), \n",
    "                                sr=16000\n",
    "                            )\n",
    "                            \n",
    "                        except Exception as s3_error:\n",
    "                            print(f\"❌ S3 ERROR for {s3_key}: {s3_error}\")\n",
    "                            errors += 1\n",
    "                            if errors > 10:  # Stop after too many errors\n",
    "                                raise Exception(f\"Too many S3 errors ({errors}). Check your S3 setup!\")\n",
    "                            continue\n",
    "                            \n",
    "                    elif isinstance(audio_path, str):\n",
    "                        # Local file path\n",
    "                        try:\n",
    "                            audio_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "                        except Exception as file_error:\n",
    "                            print(f\"❌ FILE ERROR for {audio_path}: {file_error}\")\n",
    "                            errors += 1\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(f\"❌ INVALID AUDIO PATH TYPE: {type(audio_path)} - {audio_path}\")\n",
    "                        errors += 1\n",
    "                        continue\n",
    "                # Convert audio to input_features (mel-spectrograms)\n",
    "                input_features = feature_extractor(\n",
    "                    audio_array, \n",
    "                    sampling_rate=16000, \n",
    "                    return_tensors=\"pt\"\n",
    "                ).input_features[0]  # Remove batch dimension\n",
    "                \n",
    "                # Convert text to labels (tokenized)\n",
    "                text = str(example['text']).strip()\n",
    "                if not text:\n",
    "                    print(f\"⚠️ Empty text for sample {i}, skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                labels = tokenizer(text).input_ids\n",
    "                \n",
    "                # Create processed example\n",
    "                processed_example = {\n",
    "                    'input_features': input_features,\n",
    "                    'labels': labels,\n",
    "                    'text': text,  # Keep original for reference\n",
    "                    'duration': example.get('duration', 0.0),\n",
    "                    'source': example.get('source', 'unknown')\n",
    "                }\n",
    "                \n",
    "                processed_examples.append(processed_example)\n",
    "                \n",
    "            except Exception as process_error:\n",
    "                print(f\"❌ PROCESSING ERROR for sample {i}: {process_error}\")\n",
    "                errors += 1\n",
    "                \n",
    "                # FAIL FAST - don't create dummy data\n",
    "                if errors > 20:\n",
    "                    raise Exception(f\"TOO MANY PROCESSING ERRORS ({errors})! Check your dataset and S3 setup!\")\n",
    "        \n",
    "        # Create processed dataset\n",
    "        if processed_examples:\n",
    "            from datasets import Dataset\n",
    "            processed_dataset = Dataset.from_list(processed_examples)\n",
    "            processed_datasets[split_name] = processed_dataset\n",
    "            \n",
    "            print(f\"✅ {split_name}: {len(processed_examples)} samples processed\")\n",
    "            print(f\"   Errors: {errors}\")\n",
    "            print(f\"   Success rate: {(len(processed_examples)/(len(processed_examples)+errors)*100):.1f}%\")\n",
    "            \n",
    "            # Show sample\n",
    "            sample = processed_examples[0]\n",
    "            print(f\"   Sample input_features shape: {sample['input_features'].shape}\")\n",
    "            print(f\"   Sample labels length: {len(sample['labels'])}\")\n",
    "            print(f\"   Sample text: {sample['text'][:50]}...\")\n",
    "        else:\n",
    "            raise Exception(f\"❌ NO VALID SAMPLES PROCESSED for {split_name}! All samples failed!\")\n",
    "    \n",
    "    if not processed_datasets:\n",
    "        raise Exception(\"❌ PREPROCESSING COMPLETELY FAILED! No datasets processed successfully!\")\n",
    "    \n",
    "    from datasets import DatasetDict\n",
    "    final_datasets = DatasetDict(processed_datasets)\n",
    "    \n",
    "    print(f\"\\n🎉 PREPROCESSING COMPLETE!\")\n",
    "    print(f\"✅ Converted {sum(len(d) for d in processed_datasets.values())} total samples\")\n",
    "    print(f\"✅ Datasets now have 'input_features' and 'labels' columns for training\")\n",
    "    \n",
    "    return final_datasets\n",
    "\n",
    "# EXECUTE PREPROCESSING\n",
    "print(\"🚀 Starting dataset preprocessing...\")\n",
    "\n",
    "# Check if we have the required components\n",
    "try:\n",
    "    # Check for datasets variable\n",
    "    if 'datasets' not in locals() and 'datasets' not in globals():\n",
    "        print(\"❌ Missing 'datasets' variable!\")\n",
    "        print(\"💡 Run Cell 7: Dataset Loading first\")\n",
    "        raise Exception(\"Dataset loading required!\")\n",
    "    \n",
    "    # Check for model components - they should be created in Cell 10\n",
    "    if 'tiny_feature_extractor' not in locals() and 'tiny_feature_extractor' not in globals():\n",
    "        print(\"❌ Missing 'tiny_feature_extractor' variable!\")\n",
    "        print(\"💡 Run Cell 10: Model Configuration first\")\n",
    "        raise Exception(\"Model configuration required!\")\n",
    "        \n",
    "    if 'tiny_tokenizer' not in locals() and 'tiny_tokenizer' not in globals():\n",
    "        print(\"❌ Missing 'tiny_tokenizer' variable!\")\n",
    "        print(\"💡 Run Cell 10: Model Configuration first\") \n",
    "        raise Exception(\"Model configuration required!\")\n",
    "    \n",
    "    print(\"✅ All required variables found!\")\n",
    "    print(f\"   - datasets: {len(datasets)} splits available\")\n",
    "    print(f\"   - tiny_feature_extractor: {type(tiny_feature_extractor).__name__}\")\n",
    "    print(f\"   - tiny_tokenizer: {type(tiny_tokenizer).__name__}\")\n",
    "    \n",
    "    # Apply preprocessing - process FULL dataset\n",
    "    print(\"🚀 Processing FULL dataset (all samples)...\")\n",
    "    processed_datasets = preprocess_dataset_for_training(\n",
    "        datasets, \n",
    "        tiny_feature_extractor, \n",
    "        tiny_tokenizer,\n",
    "        max_samples=None  # Process ALL samples!\n",
    "    )\n",
    "    \n",
    "    # If test succeeds, ask user if they want to process full dataset\n",
    "    print(\"\\n✅ PREPROCESSING TEST SUCCESSFUL!\")\n",
    "    print(\"💡 Processed sample datasets successfully\")\n",
    "    print(\"📊 Ready to process full dataset or proceed with training\")\n",
    "    \n",
    "    # Replace the datasets variable with processed version\n",
    "    datasets = processed_datasets\n",
    "    \n",
    "    print(f\"\\n🎯 Variable 'datasets' updated with processed data!\")\n",
    "    print(f\"✅ Training can now proceed - datasets have 'input_features' and 'labels'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ PREPROCESSING FAILED: {e}\")\n",
    "    print(\"🚫 Cannot proceed with training until preprocessing works!\")\n",
    "    print(\"\\n💡 Debug steps:\")\n",
    "    print(\"   1. Run Cell 7: Dataset Loading (creates 'datasets' variable)\")\n",
    "    print(\"   2. Run Cell 10: Model Configuration (creates tokenizer & feature_extractor)\")\n",
    "    print(\"   3. Then run this cell again for preprocessing\")\n",
    "    print(\"   4. Check if S3 credentials are working\")\n",
    "    print(\"   5. Verify audio files exist in S3\")\n",
    "    \n",
    "    # Don't create dummy data - let it fail!\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b62787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Setting up Whisper models for training...\n",
      "🔧 Loading Whisper TINY components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Whisper TINY loaded successfully!\n",
      "   Parameters: 37,760,640\n",
      "   Trainable: 37,184,640\n",
      "\n",
      "📊 Model Configurations:\n",
      "  Whisper Tiny - Batch Size: 16\n",
      "  Whisper Base - Batch Size: 8\n",
      "  Max Length: 448\n",
      "  Learning Rate: 1e-05\n",
      "  Gradient Accumulation: 2\n",
      "\n",
      "💾 GPU Memory: 14.6 GB\n",
      "✅ Configurations optimized for ml.g4dn.2xlarge (16GB VRAM)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    WhisperForConditionalGeneration, \n",
    "    WhisperTokenizer, \n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperProcessor\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "\n",
    "@dataclass\n",
    "class WhisperModelConfig:\n",
    "    \"\"\"Configuration for Whisper models optimized for ml.g4dn.2xlarge\"\"\"\n",
    "    model_name: str\n",
    "    max_length: int = 448\n",
    "    language: str = \"km\"\n",
    "    task: str = \"transcribe\"\n",
    "    batch_size_tiny: int = 16\n",
    "    batch_size_base: int = 8\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    learning_rate: float = 1e-5\n",
    "    warmup_steps: int = 500\n",
    "    max_steps: int = 5000\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 1000\n",
    "\n",
    "class WhisperModelManager:\n",
    "    \"\"\"Manage Whisper model configurations and initialization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'tiny': WhisperModelConfig(\"openai/whisper-tiny\"),\n",
    "            'base': WhisperModelConfig(\"openai/whisper-base\")\n",
    "        }\n",
    "    \n",
    "    def load_model_components(self, model_size):\n",
    "        \"\"\"Load tokenizer, feature extractor, and model\"\"\"\n",
    "        config = self.models[model_size]\n",
    "        \n",
    "        print(f\"🔧 Loading Whisper {model_size.upper()} components...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = WhisperTokenizer.from_pretrained(\n",
    "            config.model_name, \n",
    "            language=config.language, \n",
    "            task=config.task\n",
    "        )\n",
    "        \n",
    "        # Load feature extractor\n",
    "        feature_extractor = WhisperFeatureExtractor.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Load processor (combines tokenizer and feature extractor)\n",
    "        processor = WhisperProcessor.from_pretrained(config.model_name)\n",
    "        processor.tokenizer = tokenizer\n",
    "        \n",
    "        # Load model\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Configure model for Khmer\n",
    "        model.generation_config.language = config.language\n",
    "        model.generation_config.task = config.task\n",
    "        model.generation_config.forced_decoder_ids = None\n",
    "        \n",
    "        print(f\"✅ Whisper {model_size.upper()} loaded successfully!\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"   Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "        return tokenizer, feature_extractor, processor, model, config\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = WhisperModelManager()\n",
    "\n",
    "# Load Whisper Tiny components\n",
    "print(\"🚀 Setting up Whisper models for training...\")\n",
    "tiny_tokenizer, tiny_feature_extractor, tiny_processor, tiny_model, tiny_config = model_manager.load_model_components('tiny')\n",
    "\n",
    "print(f\"\\n📊 Model Configurations:\")\n",
    "print(f\"  Whisper Tiny - Batch Size: {tiny_config.batch_size_tiny}\")\n",
    "print(f\"  Whisper Base - Batch Size: {tiny_config.batch_size_base}\")\n",
    "print(f\"  Max Length: {tiny_config.max_length}\")\n",
    "print(f\"  Learning Rate: {tiny_config.learning_rate}\")\n",
    "print(f\"  Gradient Accumulation: {tiny_config.gradient_accumulation_steps}\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\\n💾 GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(\"✅ Configurations optimized for ml.g4dn.2xlarge (16GB VRAM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e1e059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Training configuration setup complete!\n",
      "🎯 Optimized for ml.g4dn.2xlarge with 16GB VRAM\n",
      "✅ Mixed precision (FP16) enabled\n",
      "✅ Gradient checkpointing enabled for memory efficiency\n",
      "✅ Early stopping configured\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"Data collator for speech-to-text training - Fixed for input_features\"\"\"\n",
    "    \n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Handle both 'input_features' and model_input_names\n",
    "        model_input_name = self.processor.model_input_names[0] if hasattr(self.processor, 'model_input_names') else 'input_features'\n",
    "        \n",
    "        # Extract input features - handle both naming conventions\n",
    "        input_features = []\n",
    "        for feature in features:\n",
    "            if 'input_features' in feature:\n",
    "                input_features.append({'input_features': feature['input_features']})\n",
    "            elif model_input_name in feature:\n",
    "                input_features.append({model_input_name: feature[model_input_name]})\n",
    "            else:\n",
    "                raise KeyError(f\"Expected 'input_features' or '{model_input_name}' in feature dict, got keys: {list(feature.keys())}\")\n",
    "        \n",
    "        # Extract labels\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "def create_training_arguments(model_size, config, output_dir):\n",
    "    \"\"\"Create optimized training arguments for ml.g4dn.2xlarge\"\"\"\n",
    "    \n",
    "    # Adjust batch size based on model size\n",
    "    if model_size == 'tiny':\n",
    "        per_device_train_batch_size = config.batch_size_tiny\n",
    "        per_device_eval_batch_size = config.batch_size_tiny\n",
    "    else:  # base\n",
    "        per_device_train_batch_size = config.batch_size_base\n",
    "        per_device_eval_batch_size = config.batch_size_base\n",
    "    \n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        max_steps=config.max_steps,\n",
    "        gradient_checkpointing=True,  # Save memory\n",
    "        fp16=True,  # Mixed precision for faster training\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_steps=config.save_steps,\n",
    "        logging_steps=100,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",  # Use eval_loss instead of wer\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=3,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=config.max_length,\n",
    "        dataloader_num_workers=4,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"]\n",
    "    )\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"Compute WER and CER metrics with fallback for SageMaker\"\"\"\n",
    "    import jiwer\n",
    "    \n",
    "    pred_ids, label_ids = eval_preds\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute metrics using jiwer as fallback\n",
    "    try:\n",
    "        # Try to use evaluate library first\n",
    "        wer_metric = evaluate.load(\"wer\")\n",
    "        cer_metric = evaluate.load(\"cer\")\n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Evaluate library failed ({e}), using jiwer fallback...\")\n",
    "        # Use jiwer as fallback\n",
    "        try:\n",
    "            # Calculate WER using jiwer\n",
    "            wer = jiwer.wer(label_str, pred_str)\n",
    "            cer = jiwer.cer(label_str, pred_str)\n",
    "        except Exception as e2:\n",
    "            print(f\"⚠️ jiwer also failed ({e2}), using basic accuracy...\")\n",
    "            # Basic accuracy fallback\n",
    "            correct = sum(1 for p, l in zip(pred_str, label_str) if p.strip() == l.strip())\n",
    "            accuracy = correct / len(pred_str) if len(pred_str) > 0 else 0\n",
    "            wer = 1.0 - accuracy  # Approximate WER as 1 - accuracy\n",
    "            cer = wer  # Use same value for CER\n",
    "    \n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "def setup_trainer(model, tokenizer, processor, config, datasets, model_size):\n",
    "    \"\"\"Setup Seq2SeqTrainer with all components\"\"\"\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # Create training arguments\n",
    "    output_dir = f\"./whisper-{model_size}-khmer\"\n",
    "    training_args = create_training_arguments(model_size, config, output_dir)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_preds: compute_metrics(eval_preds, tokenizer),\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    return trainer, training_args\n",
    "\n",
    "print(\"⚙️ Training configuration setup complete!\")\n",
    "print(\"🎯 Optimized for ml.g4dn.2xlarge with 16GB VRAM\")\n",
    "print(\"✅ Mixed precision (FP16) enabled\")\n",
    "print(\"✅ Gradient checkpointing enabled for memory efficiency\")\n",
    "print(\"✅ Early stopping configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f400f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking Prerequisites for Whisper Training...\n",
      "==================================================\n",
      "🎯 Available Variables:\n",
      "   ✅ datasets - Dataset loading (Cell 7)\n",
      "   ✅ tiny_model - Whisper Tiny model (Cell 9)\n",
      "   ✅ tiny_tokenizer - Whisper Tiny tokenizer (Cell 9)\n",
      "   ✅ tiny_processor - Whisper Tiny processor (Cell 9)\n",
      "   ✅ tiny_config - Whisper Tiny config (Cell 9)\n",
      "   ✅ setup_trainer - Training setup function (Cell 11)\n",
      "\n",
      "🎉 All prerequisites available!\n",
      "✅ Ready to start Whisper training\n",
      "\n",
      "📊 Dataset Info:\n",
      "   train: 89,034 samples\n",
      "   validation: 9,917 samples\n",
      "   test: 9,916 samples\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ✅ Pre-Training Checklist - Run This Before Training\n",
    "print(\"🔍 Checking Prerequisites for Whisper Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if all required variables are defined\n",
    "required_vars = {\n",
    "    'datasets': 'Dataset loading (Cell 6)', \n",
    "    'tiny_model': 'Whisper Tiny model (Cell 9)', \n",
    "    'tiny_tokenizer': 'Whisper Tiny tokenizer (Cell 9)',\n",
    "    'tiny_processor': 'Whisper Tiny processor (Cell 9)',\n",
    "    'tiny_config': 'Whisper Tiny config (Cell 9)',\n",
    "    'setup_trainer': 'Training setup function (Cell 10)'\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "available_vars = []\n",
    "\n",
    "for var_name, description in required_vars.items():\n",
    "    try:\n",
    "        if var_name in locals() or var_name in globals():\n",
    "            available_vars.append(f\"✅ {var_name} - {description}\")\n",
    "        else:\n",
    "            missing_vars.append(f\"❌ {var_name} - {description}\")\n",
    "    except:\n",
    "        missing_vars.append(f\"❌ {var_name} - {description}\")\n",
    "\n",
    "# Show status\n",
    "print(\"🎯 Available Variables:\")\n",
    "for var in available_vars:\n",
    "    print(f\"   {var}\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n⚠️ Missing Variables ({len(missing_vars)}):\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   {var}\")\n",
    "    \n",
    "    print(f\"\\n💡 Before training, you need to run these cells in order:\")\n",
    "    print(f\"   1. Cell 3: SageMaker Setup\")\n",
    "    print(f\"   2. Cell 5: Package Installation\") \n",
    "    print(f\"   3. Cell 6: Dataset Loading\")\n",
    "    print(f\"   4. Cell 9: Model Configuration\")\n",
    "    print(f\"   5. Cell 10: Training Setup\")\n",
    "    print(f\"   6. Then you can run the training cells\")\n",
    "    \n",
    "    print(f\"\\n🚫 Cannot start training - missing prerequisites!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n🎉 All prerequisites available!\")\n",
    "    print(f\"✅ Ready to start Whisper training\")\n",
    "    \n",
    "    # Show some stats if datasets is available\n",
    "    try:\n",
    "        if 'datasets' in locals() or 'datasets' in globals():\n",
    "            print(f\"\\n📊 Dataset Info:\")\n",
    "            for split in datasets.keys():\n",
    "                print(f\"   {split}: {len(datasets[split]):,} samples\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b46b4",
   "metadata": {},
   "source": [
    "## 7. Fine-tune Whisper Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80873dd7",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Baseline Evaluation (Run this to get WER before fine-tuning)\n",
    "print(\"🚀 BASELINE EVALUATION - Original Whisper Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate original models on your dataset\n",
    "baseline_results = []\n",
    "\n",
    "try:\n",
    "    # Load original Whisper Tiny\n",
    "    print(\"📊 Evaluating original Whisper Tiny...\")\n",
    "    orig_tiny_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "    orig_tiny_processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"km\", task=\"transcribe\")\n",
    "    \n",
    "    # Configure for Khmer\n",
    "    orig_tiny_model.generation_config.language = \"km\"\n",
    "    orig_tiny_model.generation_config.task = \"transcribe\"\n",
    "    orig_tiny_model.generation_config.forced_decoder_ids = None\n",
    "    \n",
    "    baseline_tiny_results, baseline_tiny_preds, baseline_tiny_refs = evaluator.evaluate_model(\n",
    "        orig_tiny_model, orig_tiny_processor, datasets[\"test\"], \"Original Whisper Tiny\"\n",
    "    )\n",
    "    baseline_results.append(baseline_tiny_results)\n",
    "    \n",
    "    print(f\"✅ Original Tiny WER: {baseline_tiny_results['wer']*100:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Baseline Tiny evaluation failed: {e}\")\n",
    "\n",
    "try:\n",
    "    # Load original Whisper Base\n",
    "    print(\"📊 Evaluating original Whisper Base...\")\n",
    "    orig_base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "    orig_base_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"km\", task=\"transcribe\")\n",
    "    \n",
    "    # Configure for Khmer\n",
    "    orig_base_model.generation_config.language = \"km\"\n",
    "    orig_base_model.generation_config.task = \"transcribe\" \n",
    "    orig_base_model.generation_config.forced_decoder_ids = None\n",
    "    \n",
    "    baseline_base_results, baseline_base_preds, baseline_base_refs = evaluator.evaluate_model(\n",
    "        orig_base_model, orig_base_processor, datasets[\"test\"], \"Original Whisper Base\"\n",
    "    )\n",
    "    baseline_results.append(baseline_base_results)\n",
    "    \n",
    "    print(f\"✅ Original Base WER: {baseline_base_results['wer']*100:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Baseline Base evaluation failed: {e}\")\n",
    "\n",
    "# Show baseline results with graphs\n",
    "if baseline_results:\n",
    "    print(\"\\n🎯 BASELINE RESULTS:\")\n",
    "    baseline_df = evaluator.compare_models(baseline_results)\n",
    "    \n",
    "    # Save baseline for comparison later\n",
    "    evaluator.baseline_results = baseline_results\n",
    "    \n",
    "    print(\"\\n💡 These are the baseline WER results before fine-tuning!\")\n",
    "    print(\"📈 After training, you can compare fine-tuned vs original models\")\n",
    "else:\n",
    "    print(\"❌ No baseline evaluation completed\")\n",
    "\n",
    "# Clear memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🧹 GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fbccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "class WhisperModelDeployment:\n",
    "    \"\"\"Handle model packaging and deployment to SageMaker\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket, prefix, role):\n",
    "        self.bucket = bucket\n",
    "        self.prefix = prefix\n",
    "        self.role = role\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    def create_model_tar(self, model_dir, tar_name):\n",
    "        \"\"\"Create tar.gz file for SageMaker model\"\"\"\n",
    "        print(f\"📦 Creating model archive: {tar_name}\")\n",
    "        \n",
    "        with tarfile.open(tar_name, 'w:gz') as tar:\n",
    "            for file_path in Path(model_dir).rglob('*'):\n",
    "                if file_path.is_file():\n",
    "                    arcname = file_path.relative_to(model_dir)\n",
    "                    tar.add(file_path, arcname=arcname)\n",
    "        \n",
    "        print(f\"✅ Model archive created: {tar_name}\")\n",
    "        return tar_name\n",
    "    \n",
    "    def upload_to_s3(self, file_path, s3_key):\n",
    "        \"\"\"Upload model to S3\"\"\"\n",
    "        print(f\"☁️ Uploading to S3: s3://{self.bucket}/{s3_key}\")\n",
    "        \n",
    "        self.s3_client.upload_file(file_path, self.bucket, s3_key)\n",
    "        s3_uri = f\"s3://{self.bucket}/{s3_key}\"\n",
    "        \n",
    "        print(f\"✅ Upload complete: {s3_uri}\")\n",
    "        return s3_uri\n",
    "    \n",
    "    def create_inference_script(self, model_dir):\n",
    "        \"\"\"Create inference script for SageMaker endpoint\"\"\"\n",
    "        inference_script = '''\n",
    "import torch\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model and processor\"\"\"\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_dir)\n",
    "    processor = WhisperProcessor.from_pretrained(model_dir)\n",
    "    \n",
    "    return {'model': model, 'processor': processor}\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data\"\"\"\n",
    "    if request_content_type == 'application/json':\n",
    "        input_data = json.loads(request_body)\n",
    "        \n",
    "        # Expect audio file path or base64 encoded audio\n",
    "        if 'audio_path' in input_data:\n",
    "            audio, sr = librosa.load(input_data['audio_path'], sr=16000)\n",
    "        elif 'audio_base64' in input_data:\n",
    "            import base64\n",
    "            import io\n",
    "            audio_bytes = base64.b64decode(input_data['audio_base64'])\n",
    "            audio, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)\n",
    "        else:\n",
    "            raise ValueError(\"Must provide either 'audio_path' or 'audio_base64'\")\n",
    "        \n",
    "        return audio\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"Make prediction\"\"\"\n",
    "    model = model_dict['model']\n",
    "    processor = model_dict['processor']\n",
    "    \n",
    "    # Process audio\n",
    "    input_features = processor(\n",
    "        input_data, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            max_length=448,\n",
    "            num_beams=5,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode transcription\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return {\"transcription\": transcription}\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"Format output\"\"\"\n",
    "    if content_type == 'application/json':\n",
    "        return json.dumps(prediction)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "'''\n",
    "        \n",
    "        script_path = Path(model_dir) / \"inference.py\"\n",
    "        with open(script_path, 'w') as f:\n",
    "            f.write(inference_script)\n",
    "        \n",
    "        print(f\"✅ Inference script created: {script_path}\")\n",
    "        return script_path\n",
    "    \n",
    "    def deploy_model(self, model_s3_uri, model_name, instance_type=\"ml.t2.medium\"):\n",
    "        \"\"\"Deploy model to SageMaker endpoint\"\"\"\n",
    "        print(f\"🚀 Deploying model: {model_name}\")\n",
    "        \n",
    "        # Create PyTorch model\n",
    "        pytorch_model = PyTorchModel(\n",
    "            model_data=model_s3_uri,\n",
    "            role=self.role,\n",
    "            py_version='py39',\n",
    "            framework_version='1.12',\n",
    "            entry_point='inference.py'\n",
    "        )\n",
    "        \n",
    "        # Deploy to endpoint\n",
    "        predictor = pytorch_model.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=instance_type,\n",
    "            endpoint_name=f\"{model_name}-endpoint\"\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Model deployed to endpoint: {model_name}-endpoint\")\n",
    "        return predictor\n",
    "\n",
    "# Initialize deployment manager\n",
    "deployment_manager = WhisperModelDeployment(bucket, prefix, role)\n",
    "\n",
    "# Deploy Whisper Tiny (if available)\n",
    "if os.path.exists(\"./whisper-tiny-khmer\"):\n",
    "    try:\n",
    "        print(\"🚀 Preparing Whisper Tiny for deployment...\")\n",
    "        \n",
    "        # Create inference script\n",
    "        deployment_manager.create_inference_script(\"./whisper-tiny-khmer\")\n",
    "        \n",
    "        # Create model archive\n",
    "        tiny_tar = deployment_manager.create_model_tar(\"./whisper-tiny-khmer\", \"whisper-tiny-khmer.tar.gz\")\n",
    "        \n",
    "        # Upload to S3\n",
    "        tiny_s3_key = f\"{prefix}/models/whisper-tiny-khmer.tar.gz\"\n",
    "        tiny_s3_uri = deployment_manager.upload_to_s3(tiny_tar, tiny_s3_key)\n",
    "        \n",
    "        print(f\"📍 Whisper Tiny S3 URI: {tiny_s3_uri}\")\n",
    "        \n",
    "        # Optional: Deploy to endpoint (commented out to avoid costs)\n",
    "        # tiny_predictor = deployment_manager.deploy_model(tiny_s3_uri, \"whisper-tiny-khmer\")\n",
    "        print(\"💡 To deploy endpoint, uncomment the deploy_model call\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Whisper Tiny deployment prep failed: {e}\")\n",
    "\n",
    "# Deploy Whisper Base (if available)\n",
    "if os.path.exists(\"./whisper-base-khmer\"):\n",
    "    try:\n",
    "        print(\"\\n🚀 Preparing Whisper Base for deployment...\")\n",
    "        \n",
    "        # Create inference script\n",
    "        deployment_manager.create_inference_script(\"./whisper-base-khmer\")\n",
    "        \n",
    "        # Create model archive\n",
    "        base_tar = deployment_manager.create_model_tar(\"./whisper-base-khmer\", \"whisper-base-khmer.tar.gz\")\n",
    "        \n",
    "        # Upload to S3\n",
    "        base_s3_key = f\"{prefix}/models/whisper-base-khmer.tar.gz\"\n",
    "        base_s3_uri = deployment_manager.upload_to_s3(base_tar, base_s3_key)\n",
    "        \n",
    "        print(f\"📍 Whisper Base S3 URI: {base_s3_uri}\")\n",
    "        \n",
    "        # Optional: Deploy to endpoint (commented out to avoid costs)\n",
    "        # base_predictor = deployment_manager.deploy_model(base_s3_uri, \"whisper-base-khmer\", instance_type=\"ml.m5.large\")\n",
    "        print(\"💡 To deploy endpoint, uncomment the deploy_model call\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Whisper Base deployment prep failed: {e}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"instance_type\": \"ml.g4dn.2xlarge\",\n",
    "    \"dataset\": \"Mega Khmer Dataset (160+ hours, 110K+ samples)\",\n",
    "    \"models_trained\": [],\n",
    "    \"s3_artifacts\": []\n",
    "}\n",
    "\n",
    "if os.path.exists(\"./whisper-tiny-khmer\"):\n",
    "    summary_report[\"models_trained\"].append(\"Whisper Tiny\")\n",
    "    if 'tiny_s3_uri' in locals():\n",
    "        summary_report[\"s3_artifacts\"].append({\"model\": \"Whisper Tiny\", \"uri\": tiny_s3_uri})\n",
    "\n",
    "if os.path.exists(\"./whisper-base-khmer\"):\n",
    "    summary_report[\"models_trained\"].append(\"Whisper Base\")\n",
    "    if 'base_s3_uri' in locals():\n",
    "        summary_report[\"s3_artifacts\"].append({\"model\": \"Whisper Base\", \"uri\": base_s3_uri})\n",
    "\n",
    "# Save summary report\n",
    "with open(\"training_summary.json\", \"w\") as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"\\n🎉 DEPLOYMENT PREPARATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 Models Trained: {len(summary_report['models_trained'])}\")\n",
    "print(f\"☁️ S3 Artifacts: {len(summary_report['s3_artifacts'])}\")\n",
    "print(f\"📁 S3 Bucket: {bucket}\")\n",
    "print(f\"📝 Summary saved to: training_summary.json\")\n",
    "\n",
    "if summary_report['s3_artifacts']:\n",
    "    print(\"\\n📍 Model S3 Locations:\")\n",
    "    for artifact in summary_report['s3_artifacts']:\n",
    "        print(f\"   {artifact['model']}: {artifact['uri']}\")\n",
    "\n",
    "print(\"\\n💡 Next Steps:\")\n",
    "print(\"   1. Review model performance metrics above\")\n",
    "print(\"   2. Uncomment deploy_model calls to create SageMaker endpoints\")\n",
    "print(\"   3. Use the S3 URIs for batch inference or custom deployments\")\n",
    "print(\"   4. Consider fine-tuning with additional domain-specific data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
